{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.getcwd())\n",
    "sys.path.append(\"/home/shahin/codes/improved_wgan_training-master\")\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tflib as lib\n",
    "import tflib.ops.linear\n",
    "import tflib.ops.conv2d\n",
    "import tflib.ops.batchnorm\n",
    "import tflib.ops.deconv2d\n",
    "import tflib.save_images\n",
    "import tflib.cifar10\n",
    "import tflib.inception_score\n",
    "import tflib.plot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "from showit import tile\n",
    "import functools\n",
    "import pickle\n",
    "import tensorflow.contrib.layers as tcl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_conv: 4\n",
      "WARNING:tensorflow:From /home/shahin/codes/classifier/resnet-tf/models.py:23: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/shahin/codes/classifier/resnet-tf/resnet.py:6: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/shahin/codes/classifier/resnet-tf/resnet.py:42: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download CIFAR-10 (Python version) at\n",
    "# https://www.cs.toronto.edu/~kriz/cifar.html and fill in the path to the\n",
    "# extracted files here!\n",
    "DATA_DIR = '/home/shahin/codes/datasets/cifar-10-batches-py'\n",
    "if len(DATA_DIR) == 0:\n",
    "    raise Exception('Please specify path to data directory in gan_cifar.py!')\n",
    "\n",
    "MODE = 'wgan-gp' # Valid options are dcgan, wgan, or wgan-gp\n",
    "DIM = 128 # This overfits substantially; you're probably better off with 64\n",
    "LAMBDA = 10 # Gradient penalty lambda hyperparameter\n",
    "CRITIC_ITERS = 5 # How many critic iterations per generator iteration\n",
    "BATCH_SIZE = 64 # Batch size\n",
    "ITERS = 200000 # How many generator iterations to train for\n",
    "OUTPUT_DIM = 3072 # Number of pixels in CIFAR10 (3*32*32)\n",
    "\n",
    "CONDITIONAL = False\n",
    "resume=True\n",
    "TB_save_path = 'summary_path'\n",
    "train_enc= True\n",
    "model_path='saved_model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "sess2  = tf.Session()\n",
    "sess2.run(pre_init)\n",
    "\n",
    "with open('D_Layer1_np.pickle', 'rb') as handle:\n",
    "    D_np = pickle.load(handle)\n",
    "    D_tf = tf.constant(D_np, dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "output_shape = [1, 32, 32, 3]\n",
    "D2= tf.placeholder(tf.float32, shape=(5, 5, 3, 200), name='D2')\n",
    "a2= tf.placeholder(tf.float32, shape=(1, 32, 32, 200), name='a2')\n",
    "             \n",
    "D2_np= np.ones([5, 5, 3, 200])\n",
    "D2_np= D2_np.astype(float)\n",
    "             \n",
    "a2_np=np.zeros([1, 32, 32, 200])\n",
    "a2_np[:,:,:,0]=1.0\n",
    "a2_np=a2_np.astype(float)\n",
    "             \n",
    "nrm_tf = tf.nn.conv2d_transpose(a2, D2, output_shape=output_shape, strides=[1,1,1,1], padding='SAME')\n",
    "nrm_np = sess2.run(nrm_tf, feed_dict={D2:D2_np, a2:a2_np})\n",
    "      \n",
    "nrm = tf.constant(nrm_np, dtype=tf.float32)\n",
    "print('nrm.shape')\n",
    "print(nrm_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uppercase local vars:\n",
      "\tBATCH_SIZE: 64\n",
      "\tCONDITIONAL: False\n",
      "\tCRITIC_ITERS: 5\n",
      "\tD2: Tensor(\"D2:0\", shape=(5, 5, 3, 200), dtype=float32)\n",
      "\tDATA_DIR: /home/shahin/codes/datasets/cifar-10-batches-py\n",
      "\tDIM: 128\n",
      "\tITERS: 200000\n",
      "\tLAMBDA: 10\n",
      "\tMODE: wgan-gp\n",
      "\tOUTPUT_DIM: 3072\n"
     ]
    }
   ],
   "source": [
    "lib.print_model_settings(locals().copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeakyReLU(x, alpha=0.2):\n",
    "    return tf.maximum(alpha*x, x)\n",
    "\n",
    "def ReLULayer(name, n_in, n_out, inputs):\n",
    "    output = lib.ops.linear.Linear(name+'.Linear', n_in, n_out, inputs)\n",
    "    return tf.nn.relu(output)\n",
    "\n",
    "def LeakyReLULayer(name, n_in, n_out, inputs):\n",
    "    output = lib.ops.linear.Linear(name+'.Linear', n_in, n_out, inputs)\n",
    "    return LeakyReLU(output)\n",
    "    \n",
    "    \n",
    "D=tf.get_variable(\"Generator.Dict\", dtype=tf.float32, trainable=True, initializer=D_tf)\n",
    "def Generator(n_samples, noise=None):\n",
    "    if noise is None:\n",
    "        noise = tf.random_normal([n_samples, DIM])\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    output = lib.ops.linear.Linear('Generator.Input', 128, 4*4*4*DIM, noise)\n",
    "    output = tf.reshape(output, [-1, 4*DIM, 4, 4])\n",
    "    output = lib.ops.batchnorm.Batchnorm('Generator.BN1', [0,2,3], output)  #(_, 512, 4, 4)\n",
    "    output = tf.nn.relu(output)\n",
    "\n",
    "\n",
    "    output = lib.ops.deconv2d.Deconv2D('Generator.2', 4*DIM, 4*DIM, 5, output)\n",
    "    output = lib.ops.batchnorm.Batchnorm('Generator.BN2', [0,2,3], output)   #(_, 512, 8, 8)\n",
    "    output = tf.nn.relu(output)\n",
    "\n",
    "    output = lib.ops.deconv2d.Deconv2D('Generator.3', 4*DIM, 8*DIM, 5, output)\n",
    "    output = lib.ops.batchnorm.Batchnorm('Generator.BN3', [0,2,3], output)   #(_, 1024, 16, 16)\n",
    "    output = tf.nn.relu(output)\n",
    "\n",
    "    output = lib.ops.deconv2d.Deconv2D('Generator.4', 8*DIM, 200, 5, output)\n",
    "    output = lib.ops.batchnorm.Batchnorm('Generator.BN4', [0,2,3], output)   #(_, 200, 32, 32)\n",
    "    \n",
    "    output = tf.transpose(output, perm=[0, 2, 3, 1])\n",
    "\n",
    "#    output = tf.tanh(output)\n",
    "\n",
    "    # Keeping K highest valeuse\n",
    "    k=200\n",
    "    top_values, _ = tf.nn.top_k(output, k)\n",
    "    threshold=tf.reduce_min(top_values, reduction_indices=[3], keep_dims=True)\n",
    "\n",
    "    mask=(tf.sign(tf.sign(output-threshold)+0.1)+1)/2\n",
    "    output=tf.multiply(output, mask, name=None)\n",
    "\n",
    "\n",
    "    output_shape = [n_samples, 32, 32, 3]\n",
    "\n",
    "    sparsity= tf.cast(tf.count_nonzero(output),tf.int32) /tf.size(output)\n",
    "    output = tf.nn.conv2d_transpose(output, tf.reshape(D,[5, 5, 3,-1]), output_shape=output_shape, strides=[1,1,1,1], padding='SAME')\n",
    "    #output = tf.transpose(output, perm=[0, 3, 1, 2])\n",
    "    \n",
    "    output= tf.divide(output, nrm)\n",
    "    output= tf.tanh(output)\n",
    "\n",
    "    return tf.reshape(output, [-1, OUTPUT_DIM])\n",
    "    \n",
    "    \n",
    "def Discriminator(inputs):\n",
    "    output = tf.reshape(inputs, [-1, 3, 32, 32])\n",
    "\n",
    "    output = lib.ops.conv2d.Conv2D('Discriminator.1', 3, DIM, 5, output, stride=2)\n",
    "    output = LeakyReLU(output)\n",
    "\n",
    "    output = lib.ops.conv2d.Conv2D('Discriminator.2', DIM, 2*DIM, 5, output, stride=2)\n",
    "    if MODE != 'wgan-gp':\n",
    "        output = lib.ops.batchnorm.Batchnorm('Discriminator.BN2', [0,2,3], output)\n",
    "    output = LeakyReLU(output)\n",
    "\n",
    "    output = lib.ops.conv2d.Conv2D('Discriminator.3', 2*DIM, 4*DIM, 5, output, stride=2)\n",
    "    if MODE != 'wgan-gp':\n",
    "        output = lib.ops.batchnorm.Batchnorm('Discriminator.BN3', [0,2,3], output)\n",
    "    output = LeakyReLU(output)\n",
    "\n",
    "    output = tf.reshape(output, [-1, 4*4*4*DIM])\n",
    "    output = lib.ops.linear.Linear('Discriminator.Output', 4*4*4*DIM, 1, output)\n",
    "\n",
    "    return tf.reshape(output, [-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlinearity(x):\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def Normalize(name, inputs,labels=None):\n",
    "    \"\"\"This is messy, but basically it chooses between batchnorm, layernorm, \n",
    "    their conditional variants, or nothing, depending on the value of `name` and\n",
    "    the global hyperparam flags.\"\"\"\n",
    "    if not CONDITIONAL:\n",
    "        labels = None\n",
    "    if CONDITIONAL and ACGAN and ('Discriminator' in name):\n",
    "        labels = None\n",
    "\n",
    "    if ('Discriminator' in name) and NORMALIZATION_D:\n",
    "        return lib.ops.layernorm.Layernorm(name,[1,2,3],inputs,labels=labels,n_labels=10)\n",
    "    elif ('Generator' in name) and NORMALIZATION_G:\n",
    "        if labels is not None:\n",
    "            return lib.ops.cond_batchnorm.Batchnorm(name,[0,2,3],inputs,labels=labels,n_labels=10)\n",
    "        else:\n",
    "            return lib.ops.batchnorm.Batchnorm(name,[0,2,3],inputs,fused=True)\n",
    "    else:\n",
    "        return inputs\n",
    "\n",
    "\n",
    "def ConvMeanPool(name, input_dim, output_dim, filter_size, inputs, he_init=True, biases=True):\n",
    "    output = lib.ops.conv2d.Conv2D(name, input_dim, output_dim, filter_size, inputs, he_init=he_init, biases=biases)\n",
    "    output = tf.add_n([output[:,:,::2,::2], output[:,:,1::2,::2], output[:,:,::2,1::2], output[:,:,1::2,1::2]]) / 4.\n",
    "    return output\n",
    "\n",
    "def MeanPoolConv(name, input_dim, output_dim, filter_size, inputs, he_init=True, biases=True):\n",
    "    output = inputs\n",
    "    output = tf.add_n([output[:,:,::2,::2], output[:,:,1::2,::2], output[:,:,::2,1::2], output[:,:,1::2,1::2]]) / 4.\n",
    "    output = lib.ops.conv2d.Conv2D(name, input_dim, output_dim, filter_size, output, he_init=he_init, biases=biases)\n",
    "    return output\n",
    "\n",
    "def UpsampleConv(name, input_dim, output_dim, filter_size, inputs, he_init=True, biases=True):\n",
    "    output = inputs\n",
    "    output = tf.concat([output, output, output, output], axis=1)\n",
    "    output = tf.transpose(output, [0,2,3,1])\n",
    "    output = tf.depth_to_space(output, 2)\n",
    "    output = tf.transpose(output, [0,3,1,2])\n",
    "    output = lib.ops.conv2d.Conv2D(name, input_dim, output_dim, filter_size, output, he_init=he_init, biases=biases)\n",
    "    return output\n",
    "\n",
    "def ResidualBlock(name, input_dim, output_dim, filter_size, inputs, resample=None, no_dropout=False, labels=None):\n",
    "    \"\"\"\n",
    "    resample: None, 'down', or 'up'\n",
    "    \"\"\"\n",
    "    if resample=='down':\n",
    "        conv_1        = functools.partial(lib.ops.conv2d.Conv2D, input_dim=input_dim, output_dim=input_dim)\n",
    "        conv_2        = functools.partial(ConvMeanPool, input_dim=input_dim, output_dim=output_dim)\n",
    "        conv_shortcut = ConvMeanPool\n",
    "    elif resample=='up':\n",
    "        conv_1        = functools.partial(UpsampleConv, input_dim=input_dim, output_dim=output_dim)\n",
    "        conv_shortcut = UpsampleConv\n",
    "        conv_2        = functools.partial(lib.ops.conv2d.Conv2D, input_dim=output_dim, output_dim=output_dim)\n",
    "    elif resample==None:\n",
    "        conv_shortcut = lib.ops.conv2d.Conv2D\n",
    "        conv_1        = functools.partial(lib.ops.conv2d.Conv2D, input_dim=input_dim, output_dim=output_dim)\n",
    "        conv_2        = functools.partial(lib.ops.conv2d.Conv2D, input_dim=output_dim, output_dim=output_dim)\n",
    "    else:\n",
    "        raise Exception('invalid resample value')\n",
    "\n",
    "    if output_dim==input_dim and resample==None:\n",
    "        shortcut = inputs # Identity skip-connection\n",
    "    else:\n",
    "        shortcut = conv_shortcut(name+'.Shortcut', input_dim=input_dim, output_dim=output_dim, filter_size=1, he_init=False, biases=True, inputs=inputs)\n",
    "\n",
    "    output = inputs\n",
    "    output = Normalize(name+'.N1', output, labels=labels)\n",
    "    output = nonlinearity(output)\n",
    "    output = conv_1(name+'.Conv1', filter_size=filter_size, inputs=output)    \n",
    "    output = Normalize(name+'.N2', output, labels=labels)\n",
    "    output = nonlinearity(output)            \n",
    "    output = conv_2(name+'.Conv2', filter_size=filter_size, inputs=output)\n",
    "\n",
    "    return shortcut + output\n",
    "    \n",
    "    \n",
    "def Enoder(inputs, labels=None):\n",
    "    inputs = tf.reshape(inputs, [-1, 3, 32, 32])\n",
    "    interm = ResidualBlock('Encoder.1', 3, 16, 3, inputs, resample=None, labels=labels)  #(_,128, 16, 16)\n",
    "    interm = ResidualBlock('Encoder.2', 16, 32, 3, interm, resample=None, labels=labels)  #(_,128, 16, 16)\n",
    "    interm = ResidualBlock('Encoder.3', 32, 64, 3, interm, resample=None, labels=labels)  #(_,128, 16, 16)\n",
    "    output1 = ResidualBlock('Encoder.4', 64, DIM, 3, interm, resample='down', labels=labels)  #(_,128, 16, 16)*\n",
    "    \n",
    "    interm = ResidualBlock('Encoder.5', 128, 256, 3, output1, resample=None, labels=labels)  #(_,128, 16, 16)\n",
    "    interm = ResidualBlock('Encoder.6', 256, 128, 3, interm, resample=None, labels=labels)  #(_,128, 16, 16)\n",
    "    output2 = ResidualBlock('Encoder.7', DIM, 256, 3, interm, resample='down', labels=labels)#(_,256, 8, 8)*\n",
    "    \n",
    "    interm = ResidualBlock('Encoder.8', 256, 512, 3, output2, resample=None, labels=labels)  #(_,128, 16, 16)\n",
    "    interm = ResidualBlock('Encoder.9', 512, 256, 3, interm, resample=None, labels=labels)  #(_,128, 16, 16)    \n",
    "    output3 = ResidualBlock('Encoder.10', 256, 512, 3, interm, resample='down', labels=labels)  #(_,512, 4, 4)*\n",
    "    \n",
    "    interm = nonlinearity(output3)\n",
    "    interm = tf.reshape(interm, [-1, 512*4*4])\n",
    "    interm = lib.ops.linear.Linear('Encoder.11', 512*4*4, DIM, interm)\n",
    "    output = lib.ops.batchnorm.Batchnorm('Encoder.12', [0], interm)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/shahin/codes/improved_wgan_training-master/tflib/ops/batchnorm.py:30: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-5-046cb0fb54db>:45: calling reduce_min_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "noise = tf.random_normal([BATCH_SIZE, DIM])\n",
    "\n",
    "real_data_int = tf.placeholder(tf.int32, shape=[BATCH_SIZE, OUTPUT_DIM])\n",
    "real_data = 2*((tf.cast(real_data_int, tf.float32)/255.)-.5)\n",
    "fake_data = Generator(BATCH_SIZE, noise)\n",
    "\n",
    "disc_real = Discriminator(real_data)\n",
    "disc_fake = Discriminator(fake_data)\n",
    "\n",
    "gen_params = lib.params_with_name('Generator')\n",
    "disc_params = lib.params_with_name('Discriminator')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_fake = Enoder(fake_data)\n",
    "\n",
    "enc_params = lib.params_with_name('Encoder')\n",
    "\n",
    "enc_inv_gen_cost_list=[]\n",
    "\n",
    "\n",
    "enc_inv_gen_cost_list.append(tf.reduce_mean(\n",
    "            tf.norm( tf.reshape(enc_fake - noise, [BATCH_SIZE,-1]), ord=2, axis=1) , 0))\n",
    "\n",
    "\n",
    "#enc_inv_gen_cost=tf.math.add_n(enc_inv_gen_cost_list)\n",
    "enc_inv_gen_cost= (enc_inv_gen_cost_list[0])\n",
    "\n",
    "enc_inv_gen_train_enc_adam =tf.train.AdamOptimizer(learning_rate=1e-4, beta1=0.5, beta2=0.9)\n",
    "enc_inv_gen_train_enc_op = enc_inv_gen_train_enc_adam.minimize(enc_inv_gen_cost, var_list=enc_params) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_fake = Generator(BATCH_SIZE, enc_fake)\n",
    "\n",
    "enc_real = Enoder(real_data)\n",
    "recon_real = Generator(BATCH_SIZE, enc_real)\n",
    "\n",
    "gen_recon_cost_list=[]\n",
    "\n",
    "#gen_recon_cost_list.append((1/64)* tf.reduce_mean(\n",
    "#            tf.norm( tf.reshape(enc_recon_inter[0]-gen_recon_inter[2], [BATCH_SIZE,-1]), ord=2, axis=1) , 0))\n",
    "#gen_recon_cost_list.append((1/32)* tf.reduce_mean(\n",
    "#            tf.norm( tf.reshape(enc_recon_inter[1]-gen_recon_inter[1], [BATCH_SIZE,-1]), ord=2, axis=1) , 0))\n",
    "#gen_recon_cost_list.append((1/16)* tf.reduce_mean(\n",
    "#            tf.norm( tf.reshape(enc_recon_inter[2]-gen_recon_inter[0], [BATCH_SIZE,-1]), ord=2, axis=1) , 0))\n",
    "gen_recon_cost_list.append(tf.reduce_mean(\n",
    "            tf.norm( tf.reshape(real_data - recon_real, [BATCH_SIZE,-1]), ord=2, axis=1) , 0))\n",
    "\n",
    "\n",
    "#enc_inv_gen_cost=tf.math.add_n(enc_inv_gen_cost_list)\n",
    "gen_recon_cost= (\n",
    "#                 gen_recon_cost_list[0] +\n",
    "#                 gen_recon_cost_list[1] +\n",
    "#                 gen_recon_cost_list[2] +\n",
    "                 gen_recon_cost_list[0]\n",
    ")\n",
    "\n",
    "gen_recon_cost_adam =tf.train.AdamOptimizer(learning_rate=1e-5, beta1=0.5, beta2=0.9)\n",
    "gen_recon_cost_op = gen_recon_cost_adam.minimize(gen_recon_cost, var_list=gen_params) \n",
    "\n",
    "#----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/shahin/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "if MODE == 'wgan':\n",
    "    gen_cost = -tf.reduce_mean(disc_fake)\n",
    "    disc_cost = tf.reduce_mean(disc_fake) - tf.reduce_mean(disc_real)\n",
    "\n",
    "    gen_train_op = tf.train.RMSPropOptimizer(learning_rate=5e-5).minimize(gen_cost, var_list=gen_params)\n",
    "    disc_train_op = tf.train.RMSPropOptimizer(learning_rate=5e-5).minimize(disc_cost, var_list=disc_params)\n",
    "\n",
    "    clip_ops = []\n",
    "    for var in disc_params:\n",
    "        clip_bounds = [-.01, .01]\n",
    "        clip_ops.append(\n",
    "            tf.assign(\n",
    "                var, \n",
    "                tf.clip_by_value(var, clip_bounds[0], clip_bounds[1])\n",
    "            )\n",
    "        )\n",
    "    clip_disc_weights = tf.group(*clip_ops)\n",
    "\n",
    "elif MODE == 'wgan-gp':\n",
    "    # Standard WGAN loss\n",
    "    gen_cost = -tf.reduce_mean(disc_fake)\n",
    "    disc_cost = tf.reduce_mean(disc_fake) - tf.reduce_mean(disc_real)\n",
    "\n",
    "    # Gradient penalty\n",
    "    alpha = tf.random_uniform(\n",
    "        shape=[BATCH_SIZE,1], \n",
    "        minval=0.,\n",
    "        maxval=1.\n",
    "    )\n",
    "    differences = fake_data - real_data\n",
    "    interpolates = real_data + (alpha*differences)\n",
    "    gradients = tf.gradients(Discriminator(interpolates), [interpolates])[0]\n",
    "    slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[1]))\n",
    "    gradient_penalty = tf.reduce_mean((slopes-1.)**2)\n",
    "    disc_cost += LAMBDA*gradient_penalty\n",
    "    \n",
    "    \n",
    "    #gen_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope='Generator')\n",
    "    \n",
    "    gen_train_op = tf.train.AdamOptimizer(learning_rate=1e-4, beta1=0.5, beta2=0.9).minimize(gen_cost, var_list=gen_params)\n",
    "    disc_train_op = tf.train.AdamOptimizer(learning_rate=1e-4, beta1=0.5, beta2=0.9).minimize(disc_cost, var_list=disc_params)\n",
    "\n",
    "elif MODE == 'dcgan':\n",
    "    gen_cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(disc_fake, tf.ones_like(disc_fake)))\n",
    "    disc_cost =  tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(disc_fake, tf.zeros_like(disc_fake)))\n",
    "    disc_cost += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(disc_real, tf.ones_like(disc_real)))\n",
    "    disc_cost /= 2.\n",
    "\n",
    "    gen_train_op = tf.train.AdamOptimizer(learning_rate=2e-5, beta1=0.5).minimize(gen_cost,\n",
    "                                                            var_list=gen_params)\n",
    "    disc_train_op = tf.train.AdamOptimizer(learning_rate=2e-5, beta1=0.5).minimize(disc_cost,\n",
    "                                                            var_list=disc_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For generating samples\n",
    "fixed_noise_128 = tf.constant(np.random.normal(size=(128, 128)).astype('float32'))\n",
    "fixed_noise_samples_128 = Generator(128, noise=fixed_noise_128)\n",
    "def generate_image(frame, true_dist):\n",
    "    samples = session.run(fixed_noise_samples_128 )\n",
    "    samples = ((samples+1.)*(255./2)).astype('int32')\n",
    "    lib.save_images.save_images(samples.reshape((128, 3, 32, 32)), 'samples_{}.jpg'.format(frame))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For calculating inception score\n",
    "samples_100 = Generator(100)\n",
    "def get_inception_score():\n",
    "    all_samples = []\n",
    "    for i in range(10):\n",
    "        all_samples.append(session.run(samples_100))\n",
    "    all_samples = np.concatenate(all_samples, axis=0)\n",
    "    all_samples = ((all_samples+1.)*(255./2)).astype('int32')\n",
    "    all_samples = all_samples.reshape((-1, 3, 32, 32)).transpose(0,2,3,1)\n",
    "    return lib.inception_score.get_inception_score(list(all_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset iterators\n",
    "train_gen, dev_gen = lib.cifar10.load(BATCH_SIZE, data_dir=DATA_DIR)\n",
    "def inf_train_gen():\n",
    "    while True:\n",
    "        for images,_ in train_gen():\n",
    "            yield images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_enc_loss(enc_inv_gen_cost_list_np_Arx, enc_inv_gen_cost_np_Arx, itr):\n",
    "    \n",
    "    enc_inv_gen_cost_list_np_Arx=np.array(enc_inv_gen_cost_list_np_Arx)\n",
    "    fig, axs = plt.subplots(1, 1, constrained_layout=True, squeeze=True, figsize=(16, 4))\n",
    "    \n",
    "    axs.plot(enc_inv_gen_cost_np_Arx)\n",
    "    axs.set_xlabel('Iteration')\n",
    "    axs.set_title('Total loss')\n",
    "    axs.set_ylabel('Loss')   \n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-15-56f077d6b1db>:1: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "Restoring the model:\n",
      "WARNING:tensorflow:From /home/shahin/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from saved_model/wgan_impr\n",
      "Model is restored.\n",
      "iter 0\ttrain disc cost\t-3.24399995803833\ttime\t1.7336\n",
      "iter 1\ttrain disc cost\t-2.828200101852417\ttime\t1.943\n",
      "iter 2\ttrain disc cost\t-2.3468000888824463\ttime\t0.6649\n",
      "iter 3\ttrain disc cost\t-2.4784998893737793\ttime\t0.6596\n",
      "iter 4\ttrain disc cost\t-2.623699903488159\ttime\t0.6608\n",
      "iter 99\ttrain disc cost\t-2.829158067703247\ttime\t0.6775410526315788\tdev disc cost\t-2.405400037765503\n",
      "iter 199\ttrain disc cost\t-2.777316093444824\ttime\t0.6972230000000001\tdev disc cost\t-2.2535998821258545\n",
      "iter 299\ttrain disc cost\t-2.817582607269287\ttime\t0.7033010000000001\tdev disc cost\t-2.2067999839782715\n",
      "iter 399\ttrain disc cost\t-2.8344337940216064\ttime\t0.7065469999999999\tdev disc cost\t-2.270400047302246\n",
      "iter 499\ttrain disc cost\t-2.8267836570739746\ttime\t0.7070050000000001\tdev disc cost\t-2.2597999572753906\n",
      "iter 599\ttrain disc cost\t-2.782118797302246\ttime\t0.7064400000000001\tdev disc cost\t-2.313800096511841\n"
     ]
    }
   ],
   "source": [
    "D_update=tf.assign(D, tf.nn.l2_normalize(D, dim=0))\n",
    "# Train loop\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as session:\n",
    "    \n",
    "    enc_iters=100\n",
    "    gen_inv_iters=100\n",
    "    if resume==True:\n",
    "        print('Restoring the model:')\n",
    "        #saver = tf.train.import_meta_graph('saved_model/wgan_impr.meta')\n",
    "        saver.restore(session, tf.train.latest_checkpoint(model_path))\n",
    "        print('Model is restored.')\n",
    "        \n",
    "    else:\n",
    "        session.run(tf.initialize_all_variables())\n",
    "    \n",
    "    #initialize the encoder \n",
    "    #tf.variables_initializer(enc_params)\n",
    "    \n",
    "    gen = inf_train_gen()\n",
    "\n",
    "    for iteration in range(ITERS):\n",
    "        start_time = time.time()\n",
    "            \n",
    "        # Train generator\n",
    "        if iteration > 0:\n",
    "            _ = session.run(gen_train_op)\n",
    "            _ = session.run(D_update)\n",
    "        # Train critic\n",
    "        if MODE == 'dcgan':\n",
    "            disc_iters = 1\n",
    "        else:\n",
    "            disc_iters = CRITIC_ITERS\n",
    "        for i in range(disc_iters):\n",
    "            _data = next(gen)\n",
    "            _disc_cost, _ = session.run([disc_cost, disc_train_op], \n",
    "                                        feed_dict={real_data_int: _data})\n",
    "            if MODE == 'wgan':\n",
    "                _ = session.run(clip_disc_weights)\n",
    "\n",
    "        lib.plot.plot('train disc cost', round(_disc_cost,4) )\n",
    "        lib.plot.plot('time', round(time.time() - start_time,4) )\n",
    "\n",
    "        # Calculate inception score every 1K iters\n",
    "        if iteration % 1000 == 999:\n",
    "            inception_score = get_inception_score()\n",
    "            lib.plot.plot('inception score', inception_score[0])\n",
    "            print('Saving the Model:')\n",
    "            saver.save(session, model_path+'wgan_impr')\n",
    "            print('Model is saved.')\n",
    "            \n",
    "        # Calculate the encoder score every 1K iters\n",
    "        if ((iteration % 1000 == 999) and train_enc and (inception_score[0]>0.0)):\n",
    "            print(\"Training the Encoder\")\n",
    "            enc_inv_gen_cost_list_np_Arx=[]\n",
    "            enc_inv_gen_cost_np_Arx=[]\n",
    "            for j in range(enc_iters):\n",
    "                enc_inv_gen_cost_list_np, enc_inv_gen_cost_np, _ = session.run([enc_inv_gen_cost_list,\n",
    "                                                                                    enc_inv_gen_cost,\n",
    "                                                                            enc_inv_gen_train_enc_op])\n",
    "                \n",
    "                enc_inv_gen_cost_list_np_Arx.append(enc_inv_gen_cost_list_np)\n",
    "                enc_inv_gen_cost_np_Arx.append(enc_inv_gen_cost_np)\n",
    "                if j % 200 ==0:  print('Encoder Training:', j)\n",
    "                    \n",
    "            plot_enc_loss(enc_inv_gen_cost_list_np_Arx, enc_inv_gen_cost_np_Arx, iteration)\n",
    "            print(\"Encoder is trained\")\n",
    "            \n",
    "            print(\"===================== Reconstruction of the generated images:\")\n",
    "            #fake_data_np, recon_fake_np = session.run([fake_data_samples, recon_fake_samples],\n",
    "            #                                feed_dict={noise_pl: np.random.normal(size=[32, DIM])})\n",
    "            fake_data_np, recon_fake_np = session.run([fake_data, recon_fake])\n",
    "\n",
    "            print(\"Generated images:\")\n",
    "            fake_data_np = ((fake_data_np[0:9]+1.)*(255./2)).astype('int32')\n",
    "            tile(fake_data_np.reshape((-1, 3, 32, 32)).transpose(0,2,3,1), grid=(1,9), size=15)\n",
    "            plt.show()\n",
    "            print(\"Reconstructed images:\")\n",
    "            recon_fake_np = ((recon_fake_np[0:9]+1.)*(255./2)).astype('int32')\n",
    "            tile(recon_fake_np.reshape((-1, 3, 32, 32)).transpose(0,2,3,1), grid=(1,9), size=15)\n",
    "            plt.show()\n",
    "            \n",
    "            print(\"===================== Reconstruction of the real images:\")\n",
    "            _data = next(gen)\n",
    "            fixed_data=_data.copy()\n",
    "            recon_real_np = session.run(recon_real, feed_dict={real_data_int: fixed_data})\n",
    "            \n",
    "            print(\"Training the Generator for Invertiblity\")\n",
    "            gen_recon_cost_np_Arx=[]\n",
    "            for j in range(gen_inv_iters):\n",
    "                _data = next(gen)\n",
    "                gen_recon_cost_np, _ = session.run([gen_recon_cost, gen_recon_cost_op],\n",
    "                                                                 feed_dict={real_data_int: _data})\n",
    "                gen_recon_cost_np_Arx.append(gen_recon_cost_np)\n",
    "                if j % 400 ==0: print('Generator Invertiblity Training:', j)\n",
    "            \n",
    "            fig, axs = plt.subplots(1, 1, constrained_layout=True, squeeze=True, figsize=(16, 4))\n",
    "            axs.plot(gen_recon_cost_np_Arx)\n",
    "            axs.set_title('Generator Invertiblity Loss')\n",
    "            axs.set_xlabel('Iteration')\n",
    "            axs.set_ylabel('Loss')\n",
    "            plt.show()\n",
    "            \n",
    "            recon_real_2_np = session.run(recon_real, feed_dict={real_data_int: fixed_data})\n",
    "            \n",
    "            print(\"Real images:\")\n",
    "            #_data = ((_data[0:18]+1.)*(255./2)).astype('int32')\n",
    "            tile(fixed_data[0:9].reshape((-1, 3, 32, 32)).transpose(0,2,3,1), grid=(1,9), size=15)\n",
    "            plt.show()\n",
    "            print(\"Reconstructed images:\")\n",
    "            recon_real_np = ((recon_real_np[0:9]+1.)*(255./2)).astype('int32')\n",
    "            tile(recon_real_np.reshape((-1, 3, 32, 32)).transpose(0,2,3,1), grid=(1,9), size=15)\n",
    "            plt.show()\n",
    "            print(\"Reconstructed images after training:\")\n",
    "            recon_real_2_np = ((recon_real_2_np[0:9]+1.)*(255./2)).astype('int32')\n",
    "            tile(recon_real_2_np.reshape((-1, 3, 32, 32)).transpose(0,2,3,1), grid=(1,9), size=15)\n",
    "            plt.show()\n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "        # Calculate dev loss and generate samples every 100 iters\n",
    "        if iteration % 100 == 99:\n",
    "            dev_disc_costs = []\n",
    "            for images,_ in dev_gen():\n",
    "                _dev_disc_cost = session.run(disc_cost, feed_dict={real_data_int: images}) \n",
    "                dev_disc_costs.append(round(_dev_disc_cost, 4) )\n",
    "            lib.plot.plot('dev disc cost', round(np.mean(dev_disc_costs),4))\n",
    "            generate_image(iteration, _data)\n",
    "\n",
    "        # Save logs every 100 iters\n",
    "        if (iteration < 5) or (iteration % 100 == 99):\n",
    "            lib.plot.flush()\n",
    "\n",
    "        lib.plot.tick()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=[0,1,2,3,4,5,6]\n",
    "print(test[2:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compiler.ast import flatten\n",
    "test=[[i*2, i*2+1] for i in range(10) if i*2<15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
